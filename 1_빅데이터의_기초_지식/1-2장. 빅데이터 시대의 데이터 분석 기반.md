# 1-2. 빅데이터 시대의 데이터 분석 기반

`데이터 파이프라인`의 시스템 구성을 설명한다. 빅데이터는 먼저 `데이터 레이크`로 저장된다.  
그리고 거기서부터 일부 데이터를 `데이터 마트`로 추출한다.

빅데이터 기술이 기존의 DW와 다른 점 -> `다수의 분산 시스템을 조합`하여 `확장성이 뛰어난 데이터 처리 구조`를 만든다는 점이다.  
1-2장에서는 그 차이점에 관한 설명이다.

## 빅데이터의 기술

이 책에서 다루는 `빅데이터 기술`은 `분산 시스템을 활용`하면서 `데이터를 순차적으로 가공해 나가는 일련의 구조`이다.

### 데이터 파이프라인

> 데이터 파이프라인(Data Pipeline)이란, 일반적으로 `차례대로 전달해나가는 데이터로 구성된 시스템`이다.

빅데이터의 파이프라인은 다음 조건에 따라 변화한다.

1. 데이터를 `어디에서` 수집하는가
2. 데이터로 `무엇을` 실현하고 싶은가

처음에는 간단한 구성으로 끝나도, 하고 싶은 일이 증가함에따라 시스템은 복잡해지고 그것을 어떻게 조합시킬지가 문제가 된다.

### 데이터 수집
 
 데이터 파이프라인의 시작은 `데이터를 모으는 부분`이다.

 데이터는 여러 장소에서 발생하고 각각 다른 형태를 보인다.  
 e.g. 데이터베이스에 쓰인 거래처 데이터, 파일 서버에 축적된 로그 파일, 모바일 앱에서 모여진 이벤트 데이터 및 임베디드 장비에서 보내진 센서 데이터 등

 `데이터 전송`의 방법은 크게 두 가지가 있다.  
 1. 벌크(bulk) 형
    - 이미 `어딘가에 존재하는 데이터를 정리해 추출`하는 방법이다.
    - 데이터베이스와 파일 서버 등에서 `정기적으로 데이터를 수집`하는 데에 사용한다.
 2. 스트리밍(streaming) 형
    - `차례차례로 생성되는 데이터를 끊임없이 계속해서 보내`는 방법이다.  
    - 모바일 앱과 임베디드 장비 등에서 `널리 데이터를 수집`하는 데에 사용된다.

 #### ![#1-4](/Images/1-4.jpg)  

### 스트림 처리와 배치 처리

기존의 경우, DW에서 다루는 데이터는 주로 벌크 형 방법이 이용되었다.  
빅데이터의 세계에서는 모바일 앱 등이 증가함에 따라 스트리밍 형 방법이 주류가 되고 있다.

스트리밍 형 방법으로 받은 데이터는 실시간으로 처리하기 때문에, 이것을 `스트림 처리`라고 한다.

30분간 취합한 데이터를 집계하여 그래프로 만들기 위해서는 `시계열 데이터베이스`와 같은 실시간 처리를 지향한 데이터 베이스가 자주 사용된다.[③](#1-4)  
스트림 처리의 결과를 시계열 DB에 저장함으로써, 무슨 일이 일어나는지 즉시 알 수 있다.

스트림 처리는 장기적인 데이터 분석에는 적합하지 않은 문제가 있다.
e.g. 지난 1년간의 데이터의 데이터를 분석하려고 하면 단번에 데이터양이 수천, 수만 배로 증가할 수 있다.

장기적인 데이터 분석을 위해서는 대량의 데이터를 저장하고 처리하는 데 적합한 분산 시스템이 좋다.[④, ⑤](#1-4)  
거기에 필요한 것은 스트림 처리가 아닌, 어느 정도 정리된 데이터를 효율적으로 가공하기 위한 `배치 처리`구조다.

### 분산 스토리지

수집된 데이터는 `분산 스토리지`에 저장된다.[②, ④](#1-4)  

데이터를 저장하는 대표적인 두 가지 방법  
1. `객체 스토리지` -> 한 덩어리로 모인 데이터에 이름을 부여해서 파일로 저장, e.g. Amazon S3(클라우드 서비스)
2. `NoSQL DB` -> 많은 데이터를 읽고 쓰기 위해서 NoSQL DB 사용(성능 면에서 우수)

### 분산 데이터 처리

분산 스토리지에 저장된 데이터를 처리하는 데는 `분산 데이터 처리`의 프레임워크가 필요하다.[⑥, ⑦](#1-4)  

`분산 데이터 처리의 주 역할`은 나중에 분석하기 쉽도록 `데이터를 가공`해서 그 `결과를 외부 DB에 저장`하는 것이다.

빅데이터를 SQL로 집계하는 두 가지 방법  
1. 쿼리 엔진  
    - 분산 스토리지 상의 데이터를 SQL로 집계한다.
    - e.g. Hive
    - 현재는 Hive보다도 고속인 `대화형 쿼리 엔진`도 개발되었다.

2. 외부 DW 제품 이용
    - 분산 스토리지에서 추출한 데이터를 DW에 적합한 형식으로 변환한다.
    - 이 절차를 `ETL(Extract-Transform-Load) 프로세스`라고 한다.
    - 데이터를 `추출(extract)`하고, 그것을 `가공(transform)`한 후, DW에 `로드(load)`한다.

### 워크플로 관리

전체 데이터 파이프라인의 동작을 관리하기 위해서 `워크플로 관리` 기술을 사용한다.

`정해진 시간에 배치 처리를 스케줄대로 실행`하고, `오류가 발생한 경우에는 관리자에게 통지`하는 목적으로 사용된다.

데이터 파이프라인이 복잡해짐에 따라, `한 곳에서 제어하여 전체의 움직임을 파악`하는 것이 중요해진다.  
빅데이터의 처리에는 크고 작은 시스템 장애가 발생하므로 `오류 발생 시의 처리`와 `다시 처리하기 위한 기능`을 반드시 만들어야 한다.

![1-5](/Images/1-5.jpg)

- ETL(①)은 DB의 바깥에서 데이터를 가공한다.
- ELT(②)는 데이터를 읽어 들인 후에 가공한다.
- 이 책에서는 양 쪽을 나누지 않고 이러한 일련의 흐름을 `ETL 프로세스`라고 정의한다.

## 데이터 웨어하우스(DW)와 데이터 마트(DM)

![#1-6](/Images/1-6.jpg)

- `데이터 파이프라인의 기본형`으로, 기존 방식대로의 DW를 구축하는 프로세스

### 데이터 웨어하우스(DW)
DW는 일반적인 RDB와는 달리 `대량의 데이터를 장기 보존`하는 것에 최적화되어 있다.  

DW의 `장점`은 `정리된 데이터를 한 번에 전송하는 것`은 뛰어나다.  
하지만, `단점`은 `소량의 데이터를 자주 쓰고 읽는 것`은 적합하지 않다.

전형적인 사용 방법은 업무 시스템에서 꺼낸 데이터를 하루가 끝날 때 정리하여 쓰고, 이것을 야간 시간대에 집계해서 보고서를 작성한다.

업무 시스템을 위한 RDB나 로그 등을 저장하는 파일 서버는 `데이터 소스`라고 부른다.  
여기에 보존된 `로우 데이터(원시 데이터)`를 추출하고 필요에 따라 가공한 후 DW에 저장한다.

이 흐름이 `ETL 프로세스`다.  
DW 구축에는 `ETL 도구`라는 전용 소프트웨어가 자주 이용된다.

### 데이터 마트(DM)

DW는 중요한 데이터 처리에 사용되기 때문에 아무때나 사용해 시스템에 과부하를 초래하는 것은 곤란하다.  
따라서, `데이터 분석과 같은 목적`에 사용하는 경우에는 `DW에서 필요한 데이터만을 추출`하여 `데이터 마트(DM)`를 구축한다.

DM은 BI 도구와 조합시키는 형태로 데이터를 시각화하는 데에도 사용된다.


DW와 DM 모두 `SQL로 데이터를 집계`하기 때문에, `테이블 설계를 제대로 정한 후에 데이터를 투입`한다.  
BI 도구로 데이터를 볼 경우에는 미리 시각화에 적합항 형태로 테이블을 준비해야 한다.

**💡 따라서, DW를 중심으로 하는 파이프라인에서는 테이블 설계와 ETL 프로세스가 중요하다.**

## 데이터 레이크

